{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6bee415730f32a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "START OF THE FINAL PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c031ef52f94cbb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "All of the import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa67f091efafbc32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:40:27.441733Z",
     "start_time": "2024-04-24T20:40:26.253721Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from torch import torch_version\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd75bad8ccf5b51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1)LOAD THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3adce08b5c44c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:40:28.371651Z",
     "start_time": "2024-04-24T20:40:28.285406Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "# from config import CONFIG\n",
    "\n",
    "\n",
    "\n",
    "class PREPROCESS:\n",
    "    def __init__(self) -> None:\n",
    "        df = pd.read_csv(\"spotify_millsongdata.csv\")\n",
    "        df = df[:9]\n",
    "        df[\"prepText\"] = df[\"text\"].apply(self.cleanString)\n",
    "        df = df[[\"artist\", \"song\", \"prepText\"]]\n",
    "        df[\"allText\"] = df[\"artist\"]+\" \"+df[\"song\"] + \" \"+df[\"prepText\"]\n",
    "        # save the file \n",
    "        df.to_csv(\"preprocessed_songs.csv\", index=False)\n",
    "    \n",
    "    def cleanString(self,sentence):\n",
    "        sentence = sentence.translate(str.maketrans(\"\",\"\", string.punctuation)).strip().lower()\n",
    "        sentence = re.sub(r\"https?://\\s+\", \"\", sentence)\n",
    "        sentence = re.sub(r\"\\b\\d+\\b\",  \"\", sentence)\n",
    "        sentence = re.sub(r\" +\",\" \",sentence).replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"\\r\\n\", \"\")\n",
    "        sentence = re.sub(\"\\s+\", \" \", sentence)\n",
    "        return sentence    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791eb5383158367c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:40:29.425122Z",
     "start_time": "2024-04-24T20:40:28.617389Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PREPROCESS at 0x162acf0a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESS()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d34bcc2950e075d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:40:30.587002Z",
     "start_time": "2024-04-24T20:40:29.967951Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import faiss\n",
    "class findEmbedding:\n",
    "    def __init__(self) -> None:\n",
    "        self.df = pd.read_csv(\"preprocessed_songs.csv\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embeddings = self.computeEmbeddings()\n",
    "        if not os.path.isdir(\"results\"):\n",
    "            os.mkdir(\"results\")\n",
    "        PATH = os.path.join(\"embedding.npy\")\n",
    "        with open( PATH, \"wb\") as fp:\n",
    "            np.save(fp, embeddings)\n",
    "\n",
    "    def computeEmbeddings(self):\n",
    "        sentences = self.df[\"allText\"].values\n",
    "        embeddings = []\n",
    "        for sentence in tqdm(sentences):\n",
    "            embedding = self.model.encode(sentence)\n",
    "            embeddings.append(embedding)\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0f1d545a482278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:40:33.887534Z",
     "start_time": "2024-04-24T20:40:31.160541Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 9/9 [00:04<00:00,  1.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.findEmbedding at 0x17666ab00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50b4f33b4fb2040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:39:30.245904Z",
     "start_time": "2024-04-24T20:39:28.823554Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 9/9 [00:00<00:00, 40.57it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'faiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(artist_songs)\n\u001b[1;32m     32\u001b[0m findEmbedding()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mrecmmSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mrecmmSystem.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(PATH)\n\u001b[1;32m      7\u001b[0m dimension \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m quantize \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[38;5;241m.\u001b[39mIndexFlatL2(dimension)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexIVFFlat(quantize, dimension, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_trained:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faiss' is not defined"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "class recmmSystem:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.df = pd.read_csv(\"preprocessed_songs.csv\")\n",
    "        PATH = os.path.join( \"embedding.npy\")\n",
    "        embeddings = np.load(PATH)\n",
    "        dimension = embeddings.shape[1]\n",
    "        quantize = faiss.IndexFlatL2(dimension)\n",
    "        self.index = faiss.IndexIVFFlat(quantize, dimension, 50)\n",
    "\n",
    "        if not self.index.is_trained:\n",
    "            print(1)\n",
    "            self.index.train(embeddings)\n",
    "        \n",
    "        if self.index.is_trained:\n",
    "            print(0)\n",
    "            self.index.add(embeddings)\n",
    "\n",
    "        print(\"Total Number of embeddings index {}\".format(self.index.ntotal))\n",
    "    \n",
    "    def nearestNeighbour(self, query, k):\n",
    "        queryEmbeddings = self.model.encode([query])\n",
    "        D, I = self.index.search(queryEmbeddings, k)\n",
    "        artist_songs = dict()\n",
    "        allRelatable = self.df.iloc[I[0]]\n",
    "        for index, rows in allRelatable.iterrows():\n",
    "            print(rows[\"artist\"])\n",
    "            artist_songs[rows[\"artist\"]] = rows[\"song\"]\n",
    "        \n",
    "        print(\"Query: \", query)\n",
    "        print(artist_songs)\n",
    "findEmbedding()\n",
    "recmmSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a70095453b1e4fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:40:45.032742Z",
     "start_time": "2024-04-24T20:40:44.925249Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/hpp2kxf54pndqxk7bcfbzl6w0000gn/T/ipykernel_38932/456693216.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_tensors_tensor = torch.tensor(user_tensors, dtype=torch.float32)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 385])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'faiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;124m\"\u001b[39m, queryEmbeddings)\n\u001b[1;32m     82\u001b[0m      \u001b[38;5;28mprint\u001b[39m(artist_songs)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mnearestNeighbour\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 68\u001b[0m, in \u001b[0;36mnearestNeighbour\u001b[0;34m(queryEmbeddings, k)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnearestNeighbour\u001b[39m( queryEmbeddings, k):\n\u001b[0;32m---> 68\u001b[0m      quantize \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[38;5;241m.\u001b[39mIndexFlatL2(queryEmbeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     69\u001b[0m      index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexIVFFlat(quantize, queryEmbeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     70\u001b[0m      D, I \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(queryEmbeddings, k)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faiss' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class SongUserClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SongUserClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "num_users = 1  # Change this to the number of users you have\n",
    "user_tensors = torch.eye(num_users)  # Create one-hot encoded tensors for users\n",
    "labels = [1,1,1,1,1,0,0,0,0]\n",
    "embeddings = np.load(\"embedding.npy\")\n",
    "\n",
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "user_tensors_tensor = torch.tensor(user_tensors, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "input_tensor = torch.cat((user_tensors_tensor.repeat(len(labels), 1), embeddings_tensor), dim=1)\n",
    "print(input_tensor.shape)\n",
    "input_size = input_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 4  # Adjust the batch size as needed\n",
    "num_epochs = 100\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(input_tensor, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = SongUserClassifier(input_size, hidden_size,1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    epoch_loss = total_loss / len(dataset)\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Extract learned user embeddings\n",
    "user_embeddings = model.fc1.weight.data.numpy()\n",
    "\n",
    "# Print learned user embeddings\n",
    "# print(\"Learned User Embeddings:\")\n",
    "# print(user_embeddings)\n",
    "def nearestNeighbour( queryEmbeddings, k):\n",
    "     quantize = faiss.IndexFlatL2(queryEmbeddings.shape[1])\n",
    "     index = faiss.IndexIVFFlat(quantize, queryEmbeddings.shape[1], 5)\n",
    "     D, I = index.search(queryEmbeddings, k)\n",
    "     print(-1)\n",
    "     df = pd.read_csv(\"preprocessed_songs.csv\")\n",
    "     print(0)\n",
    "     artist_songs = dict()\n",
    "     allRelatable = df.iloc[I[0]]\n",
    "     for index, rows in allRelatable.iterrows():\n",
    "         print(1)\n",
    "         print(rows[\"artist\"])\n",
    "         artist_songs[rows[\"artist\"]] = rows[\"song\"]\n",
    "    \n",
    "     print(\"Query: \", queryEmbeddings)\n",
    "     print(artist_songs)\n",
    "nearestNeighbour(user_embeddings,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d11baff66bc67c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:22:20.108288Z",
     "start_time": "2024-04-24T02:22:20.104822Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_tensors = torch.eye(10) \n",
    "user_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd080bc7473d032",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (catalog.py, line 327)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3442\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\n\u001b[0;31m    from pyechonest import catalog\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyechonest/catalog.py:327\u001b[0;36m\u001b[0m\n\u001b[0;31m    if since:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from pyechonest import catalog\n",
    "cat = catalog.Catalog('CACNYVZ1332EB0BA9D')\n",
    "cat.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5ac65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class fs_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(fs_model, self).__init__()\n",
    "        self.relu = nn.ReLU()  \n",
    "        self.sigmoid = nn.Sigmoid();\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x);\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da8de495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss 0.24668718874454498\n",
      "Epoch 1, Batch 0, Loss 0.24620075523853302\n",
      "Epoch 2, Batch 0, Loss 0.24578191339969635\n",
      "Epoch 3, Batch 0, Loss 0.24546173214912415\n",
      "Epoch 4, Batch 0, Loss 0.2452165186405182\n",
      "Epoch 5, Batch 0, Loss 0.2450389713048935\n",
      "Epoch 6, Batch 0, Loss 0.24493326246738434\n",
      "Epoch 7, Batch 0, Loss 0.24490225315093994\n",
      "Epoch 8, Batch 0, Loss 0.24493476748466492\n",
      "Epoch 9, Batch 0, Loss 0.24500256776809692\n",
      "Epoch 10, Batch 0, Loss 0.24506627023220062\n",
      "Epoch 11, Batch 0, Loss 0.2450985610485077\n",
      "Epoch 12, Batch 0, Loss 0.24509486556053162\n",
      "Epoch 13, Batch 0, Loss 0.245061993598938\n",
      "Epoch 14, Batch 0, Loss 0.2450140416622162\n",
      "Epoch 15, Batch 0, Loss 0.24496565759181976\n",
      "Epoch 16, Batch 0, Loss 0.2449270486831665\n",
      "Epoch 17, Batch 0, Loss 0.2449045181274414\n",
      "Epoch 18, Batch 0, Loss 0.2448987513780594\n",
      "Epoch 19, Batch 0, Loss 0.24490652978420258\n",
      "Epoch 20, Batch 0, Loss 0.24492111802101135\n",
      "Epoch 21, Batch 0, Loss 0.2449357509613037\n",
      "Epoch 22, Batch 0, Loss 0.2449459284543991\n",
      "Epoch 23, Batch 0, Loss 0.24494953453540802\n",
      "Epoch 24, Batch 0, Loss 0.24494647979736328\n",
      "Epoch 25, Batch 0, Loss 0.24493834376335144\n",
      "Epoch 26, Batch 0, Loss 0.24492748081684113\n",
      "Epoch 27, Batch 0, Loss 0.2449164092540741\n",
      "Epoch 28, Batch 0, Loss 0.24490711092948914\n",
      "Epoch 29, Batch 0, Loss 0.24490100145339966\n",
      "Epoch 30, Batch 0, Loss 0.2448982298374176\n",
      "Epoch 31, Batch 0, Loss 0.24489861726760864\n",
      "Epoch 32, Batch 0, Loss 0.24490126967430115\n",
      "Epoch 33, Batch 0, Loss 0.2449047863483429\n",
      "Epoch 34, Batch 0, Loss 0.24490784108638763\n",
      "Epoch 35, Batch 0, Loss 0.2449096143245697\n",
      "Epoch 36, Batch 0, Loss 0.24490974843502045\n",
      "Epoch 37, Batch 0, Loss 0.24490834772586823\n",
      "Epoch 38, Batch 0, Loss 0.24490585923194885\n",
      "Epoch 39, Batch 0, Loss 0.2449030578136444\n",
      "Epoch 40, Batch 0, Loss 0.24490052461624146\n",
      "Epoch 41, Batch 0, Loss 0.24489879608154297\n",
      "Epoch 42, Batch 0, Loss 0.24489803612232208\n",
      "Epoch 43, Batch 0, Loss 0.24489815533161163\n",
      "Epoch 44, Batch 0, Loss 0.2448989897966385\n",
      "Epoch 45, Batch 0, Loss 0.24490001797676086\n",
      "Epoch 46, Batch 0, Loss 0.24490094184875488\n",
      "Epoch 47, Batch 0, Loss 0.24490144848823547\n",
      "Epoch 48, Batch 0, Loss 0.24490146338939667\n",
      "Epoch 49, Batch 0, Loss 0.24490106105804443\n",
      "Epoch 50, Batch 0, Loss 0.24490030109882355\n",
      "Epoch 51, Batch 0, Loss 0.2448994368314743\n",
      "Epoch 52, Batch 0, Loss 0.24489867687225342\n",
      "Epoch 53, Batch 0, Loss 0.24489815533161163\n",
      "Epoch 54, Batch 0, Loss 0.2448979914188385\n",
      "Epoch 55, Batch 0, Loss 0.24489808082580566\n",
      "Epoch 56, Batch 0, Loss 0.24489839375019073\n",
      "Epoch 57, Batch 0, Loss 0.2448986917734146\n",
      "Epoch 58, Batch 0, Loss 0.2448989450931549\n",
      "Epoch 59, Batch 0, Loss 0.24489909410476685\n",
      "Epoch 60, Batch 0, Loss 0.2448989897966385\n",
      "Epoch 61, Batch 0, Loss 0.24489879608154297\n",
      "Epoch 62, Batch 0, Loss 0.2448984980583191\n",
      "Epoch 63, Batch 0, Loss 0.24489827454090118\n",
      "Epoch 64, Batch 0, Loss 0.24489808082580566\n",
      "Epoch 65, Batch 0, Loss 0.2448979765176773\n",
      "Epoch 66, Batch 0, Loss 0.2448979914188385\n",
      "Epoch 67, Batch 0, Loss 0.24489808082580566\n",
      "Epoch 68, Batch 0, Loss 0.24489817023277283\n",
      "Epoch 69, Batch 0, Loss 0.24489827454090118\n",
      "Epoch 70, Batch 0, Loss 0.24489833414554596\n",
      "Epoch 71, Batch 0, Loss 0.24489828944206238\n",
      "Epoch 72, Batch 0, Loss 0.2448982447385788\n",
      "Epoch 73, Batch 0, Loss 0.24489815533161163\n",
      "Epoch 74, Batch 0, Loss 0.24489803612232208\n",
      "Epoch 75, Batch 0, Loss 0.2448979765176773\n",
      "Epoch 76, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 77, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 78, Batch 0, Loss 0.2448980212211609\n",
      "Epoch 79, Batch 0, Loss 0.24489803612232208\n",
      "Epoch 80, Batch 0, Loss 0.24489808082580566\n",
      "Epoch 81, Batch 0, Loss 0.24489808082580566\n",
      "Epoch 82, Batch 0, Loss 0.24489808082580566\n",
      "Epoch 83, Batch 0, Loss 0.24489803612232208\n",
      "Epoch 84, Batch 0, Loss 0.24489803612232208\n",
      "Epoch 85, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 86, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 87, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 88, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 89, Batch 0, Loss 0.2448979914188385\n",
      "Epoch 90, Batch 0, Loss 0.2448979765176773\n",
      "Epoch 91, Batch 0, Loss 0.2448979914188385\n",
      "Epoch 92, Batch 0, Loss 0.2448979914188385\n",
      "Epoch 93, Batch 0, Loss 0.2448979765176773\n",
      "Epoch 94, Batch 0, Loss 0.2448979765176773\n",
      "Epoch 95, Batch 0, Loss 0.2448979914188385\n",
      "Epoch 96, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 97, Batch 0, Loss 0.2448979616165161\n",
      "Epoch 98, Batch 0, Loss 0.2448979765176773\n",
      "Epoch 99, Batch 0, Loss 0.2448979616165161\n",
      "Accuracy on test set: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/hpp2kxf54pndqxk7bcfbzl6w0000gn/T/ipykernel_47163/3206848114.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_tensors_tensor = torch.tensor(user_tensors, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "num_users = 1  # Change this to the number of users you have\n",
    "user_tensors = torch.eye(num_users)  # Create one-hot encoded tensors for users\n",
    "labels = [1,1,1,1,1,0,0,0,0]\n",
    "embeddings = np.load(\"embedding.npy\")\n",
    "\n",
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "user_tensors_tensor = torch.tensor(user_tensors, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "input_tensor = torch.cat((user_tensors_tensor.repeat(len(labels), 1), embeddings_tensor), dim=1)\n",
    "input_size = input_tensor.shape[1]\n",
    "\n",
    "model = fs_model(input_size,50,1)\n",
    "loss_func  = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = TensorDataset(input_tensor, labels_tensor)\n",
    "data_train, data_test = torch.utils.data.random_split(dataset, [7, 2])\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch {}, Batch {}, Loss {}'.format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy on test set: {}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb4073ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/hpp2kxf54pndqxk7bcfbzl6w0000gn/T/ipykernel_47163/1662435017.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_tensors_tensor = torch.tensor(user_tensors, dtype=torch.float32)\n",
      "[I 2024-05-05 12:33:18,645] A new study created in memory with name: no-name-1b1ca86b-8eb4-4d79-a8a1-13ea012667eb\n",
      "/var/folders/_3/hpp2kxf54pndqxk7bcfbzl6w0000gn/T/ipykernel_47163/1662435017.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
      "[I 2024-05-05 12:33:18,765] Trial 0 finished with value: 50.0 and parameters: {'hidden_dim': 158, 'lr': 0.03713329722640982, 'batch_size': 64}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:18,863] Trial 1 finished with value: 50.0 and parameters: {'hidden_dim': 136, 'lr': 0.001161580230491938, 'batch_size': 64}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:18,957] Trial 2 finished with value: 50.0 and parameters: {'hidden_dim': 67, 'lr': 0.00013598362941954204, 'batch_size': 32}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:19,057] Trial 3 finished with value: 0.0 and parameters: {'hidden_dim': 165, 'lr': 0.00010106929091045225, 'batch_size': 64}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:19,153] Trial 4 finished with value: 50.0 and parameters: {'hidden_dim': 114, 'lr': 0.0007794263364055527, 'batch_size': 32}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:19,255] Trial 5 finished with value: 50.0 and parameters: {'hidden_dim': 144, 'lr': 0.00039193931916810124, 'batch_size': 128}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:19,379] Trial 6 finished with value: 50.0 and parameters: {'hidden_dim': 133, 'lr': 0.005045533339427111, 'batch_size': 128}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:19,505] Trial 7 finished with value: 50.0 and parameters: {'hidden_dim': 123, 'lr': 0.01490913455396996, 'batch_size': 64}. Best is trial 0 with value: 50.0.\n",
      "[I 2024-05-05 12:33:19,641] Trial 8 finished with value: 100.0 and parameters: {'hidden_dim': 55, 'lr': 0.022308781941034583, 'batch_size': 64}. Best is trial 8 with value: 100.0.\n",
      "[I 2024-05-05 12:33:19,775] Trial 9 finished with value: 50.0 and parameters: {'hidden_dim': 69, 'lr': 0.00746166054628492, 'batch_size': 64}. Best is trial 8 with value: 100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'hidden_dim': 55, 'lr': 0.022308781941034583, 'batch_size': 64}\n",
      "Best validation accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "num_users = 1  # Change this to the number of users you have\n",
    "user_tensors = torch.eye(num_users)  # Create one-hot encoded tensors for users\n",
    "labels = [1,1,1,1,1,0,0,0,0]\n",
    "embeddings = np.load(\"embedding.npy\")\n",
    "\n",
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "user_tensors_tensor = torch.tensor(user_tensors, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "input_tensor = torch.cat((user_tensors_tensor.repeat(len(labels), 1), embeddings_tensor), dim=1)\n",
    "input_size = input_tensor.shape[1]\n",
    "\n",
    "def training(model, train_loader, test_loader, loss, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print('Epoch {}, Batch {}, Loss {}'.format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        # print('Accuracy on test set: {}%'.format(100*correct/total))\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 50, 200)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    num_epochs = 100\n",
    "\n",
    "    model = fs_model(input_size,50,1)\n",
    "    loss_func  = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    dataset = TensorDataset(input_tensor, labels_tensor)\n",
    "    data_train, data_test = torch.utils.data.random_split(dataset, [7, 2])\n",
    "    train_loader = DataLoader(data_train, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(data_test, batch_size=32, shuffle=False)\n",
    "\n",
    "    training(model, train_loader, test_loader, loss_func, optimizer, num_epochs)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best validation accuracy:\", best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
